{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from torch import nn\n",
    "from torch.nn import functional as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch大小\n",
    "BATCH_SIZE = 2\n",
    "# 词袋模型：单词表中的单词总数\n",
    "TOKEN_CNT = 8\n",
    "# 句子的最大长度\n",
    "MAX_SENTENCE_LEN = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型维度（特征维度）大小\n",
    "MODEL_DIM = 8  # 原论文是512，演示起见，这里定义成8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 演示起见，手写训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集输入句子：[tensor([4, 5, 0, 0, 0]), tensor([2, 2, 7, 3, 0])]\n",
      "训练集标签句子：[tensor([2, 5, 5, 2, 0]), tensor([2, 2, 5, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"生成一个batch的随机句子当做训练集\n",
    "   句子由token组成，有独热编码和词袋模型两种表示token的方式\n",
    "   这里采用词袋模型，也就是一个词用一个独特的整数表示\n",
    "\"\"\"\n",
    "\n",
    "# Batch size为2，也就是2个句子\n",
    "# 输入的句子长度分别为2,4\n",
    "train_input_lengths = torch.Tensor([2, 4]).to(torch.int32)\n",
    "# 标签的句子长度分别为4,3\n",
    "train_label_lengths = torch.Tensor([4, 3]).to(torch.int32)\n",
    "\n",
    "train_input_sentences = []\n",
    "for length in train_input_lengths:\n",
    "    # 按照长度随机生成句子\n",
    "    input = torch.randint(1, TOKEN_CNT, (length,))\n",
    "    # 将句子padding到最大长度\n",
    "    input = TF.pad(input, (0, MAX_SENTENCE_LEN - length))\n",
    "    train_input_sentences.append(input)\n",
    "\n",
    "train_label_sentences = []\n",
    "for length in train_label_lengths:\n",
    "    # 按照长度随机生成句子\n",
    "    label = torch.randint(1, TOKEN_CNT, (length,))\n",
    "    # 将句子padding到最大长度\n",
    "    label = TF.pad(label, (0, MAX_SENTENCE_LEN - length))\n",
    "    train_label_sentences.append(label)\n",
    "\n",
    "print(f\"训练集输入句子：{train_input_sentences}\")\n",
    "print(f\"训练集标签句子：{train_label_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集输入矩阵：\n",
      "tensor([[2, 5, 5, 2, 0],\n",
      "        [2, 2, 5, 0, 0]])\n",
      "训练集标签矩阵：\n",
      "tensor([[2, 5, 5, 2, 0],\n",
      "        [2, 2, 5, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"把句子列表变成矩阵\n",
    "   把每个句子先变成二维矩阵，再拼接到一起成为真正的输入矩阵和标签矩阵\n",
    "\"\"\"\n",
    "\n",
    "for index, value in enumerate(train_input_sentences):\n",
    "    value = torch.squeeze(value)\n",
    "    train_input_sentences[index] = torch.unsqueeze(value, dim=0)\n",
    "\n",
    "for index, value in enumerate(train_label_sentences):\n",
    "    value = torch.squeeze(value)\n",
    "    train_label_sentences[index] = torch.unsqueeze(value, dim=0)\n",
    "\n",
    "train_input_mat = torch.cat(train_label_sentences, dim=0)\n",
    "train_label_mat = torch.cat(train_label_sentences, dim=0)\n",
    "\n",
    "print(f\"训练集输入矩阵：\\n{train_input_mat}\")\n",
    "print(f\"训练集标签矩阵：\\n{train_label_mat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 构造Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0425,  1.9791, -1.9645,  1.2165, -0.9497,  1.1116,  0.1297, -2.3036],\n",
      "        [-0.8854, -0.0480,  2.0325, -0.9215,  0.4905,  0.4984, -0.5039,  1.7998],\n",
      "        [ 1.4225, -1.4392, -1.0206,  1.2507,  1.1182, -0.3806,  0.3524, -1.0706],\n",
      "        [ 0.0544, -1.0229,  0.0530,  1.1227,  0.0159,  0.7875, -0.1771,  0.0470],\n",
      "        [-1.1928,  0.1478, -1.7844, -0.5550, -0.7484,  0.3757,  0.5352, -0.3112],\n",
      "        [-1.3368, -0.4440, -0.8991,  0.1203,  0.9217,  1.0369, -0.7691, -0.5544],\n",
      "        [ 1.6839,  2.7043, -0.1043,  1.4168, -2.1272,  0.1547, -0.8379, -0.1797],\n",
      "        [ 0.4358, -1.0582,  1.1119,  0.1677, -0.3939,  1.6558, -0.2100,  0.8067],\n",
      "        [ 0.4582, -0.3723, -0.2612,  1.2082,  0.3122, -0.7487, -0.3969, -1.3711]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2170,  1.0516,  1.5410, -1.2915, -0.8874,  0.8474,  0.2192,  0.7642],\n",
      "        [-0.0463,  1.3378,  0.4566,  0.2533,  0.5210, -1.1080,  0.3959,  0.2111],\n",
      "        [ 0.1288, -0.0469, -0.4642,  0.1670,  0.4396,  0.5697, -1.1609,  2.3318],\n",
      "        [ 0.7631,  0.3993, -0.0261,  0.1821,  0.6883, -0.0735,  0.5157, -0.1001],\n",
      "        [ 1.8112,  0.6946, -1.5403, -0.4176, -1.0389, -1.2362, -1.5412, -0.9638],\n",
      "        [-1.3129, -2.0868,  0.0827, -0.5277,  1.1171,  0.1915,  0.3160, -0.4684],\n",
      "        [ 1.8231,  0.8923,  0.0627, -1.6954, -1.1753,  0.2662,  0.8292, -1.4397],\n",
      "        [ 0.9963,  0.2899,  1.2452,  1.3090,  0.3008,  0.5252,  1.2044,  0.0277],\n",
      "        [-0.2534,  1.0823,  0.6869,  1.1548,  2.2897,  0.0630, -0.6288, -0.5686]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# num_embedding参数+1是因为多了个pad出来的0\n",
    "input_embedding_table = nn.Embedding(TOKEN_CNT + 1, MODEL_DIM)\n",
    "lable_embedding_table = nn.Embedding(TOKEN_CNT + 1, MODEL_DIM)\n",
    "\n",
    "print(input_embedding_table.weight)\n",
    "print(lable_embedding_table.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4225, -1.4392, -1.0206,  1.2507,  1.1182, -0.3806,  0.3524,\n",
      "          -1.0706],\n",
      "         [-1.3368, -0.4440, -0.8991,  0.1203,  0.9217,  1.0369, -0.7691,\n",
      "          -0.5544],\n",
      "         [-1.3368, -0.4440, -0.8991,  0.1203,  0.9217,  1.0369, -0.7691,\n",
      "          -0.5544],\n",
      "         [ 1.4225, -1.4392, -1.0206,  1.2507,  1.1182, -0.3806,  0.3524,\n",
      "          -1.0706],\n",
      "         [ 0.0425,  1.9791, -1.9645,  1.2165, -0.9497,  1.1116,  0.1297,\n",
      "          -2.3036]],\n",
      "\n",
      "        [[ 1.4225, -1.4392, -1.0206,  1.2507,  1.1182, -0.3806,  0.3524,\n",
      "          -1.0706],\n",
      "         [ 1.4225, -1.4392, -1.0206,  1.2507,  1.1182, -0.3806,  0.3524,\n",
      "          -1.0706],\n",
      "         [-1.3368, -0.4440, -0.8991,  0.1203,  0.9217,  1.0369, -0.7691,\n",
      "          -0.5544],\n",
      "         [ 0.0425,  1.9791, -1.9645,  1.2165, -0.9497,  1.1116,  0.1297,\n",
      "          -2.3036],\n",
      "         [ 0.0425,  1.9791, -1.9645,  1.2165, -0.9497,  1.1116,  0.1297,\n",
      "          -2.3036]]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[ 0.1288, -0.0469, -0.4642,  0.1670,  0.4396,  0.5697, -1.1609,\n",
      "           2.3318],\n",
      "         [-1.3129, -2.0868,  0.0827, -0.5277,  1.1171,  0.1915,  0.3160,\n",
      "          -0.4684],\n",
      "         [-1.3129, -2.0868,  0.0827, -0.5277,  1.1171,  0.1915,  0.3160,\n",
      "          -0.4684],\n",
      "         [ 0.1288, -0.0469, -0.4642,  0.1670,  0.4396,  0.5697, -1.1609,\n",
      "           2.3318],\n",
      "         [-0.2170,  1.0516,  1.5410, -1.2915, -0.8874,  0.8474,  0.2192,\n",
      "           0.7642]],\n",
      "\n",
      "        [[ 0.1288, -0.0469, -0.4642,  0.1670,  0.4396,  0.5697, -1.1609,\n",
      "           2.3318],\n",
      "         [ 0.1288, -0.0469, -0.4642,  0.1670,  0.4396,  0.5697, -1.1609,\n",
      "           2.3318],\n",
      "         [-1.3129, -2.0868,  0.0827, -0.5277,  1.1171,  0.1915,  0.3160,\n",
      "          -0.4684],\n",
      "         [-0.2170,  1.0516,  1.5410, -1.2915, -0.8874,  0.8474,  0.2192,\n",
      "           0.7642],\n",
      "         [-0.2170,  1.0516,  1.5410, -1.2915, -0.8874,  0.8474,  0.2192,\n",
      "           0.7642]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_input_embedding = input_embedding_table(train_input_mat)\n",
    "train_label_embedding = lable_embedding_table(train_label_mat)\n",
    "\n",
    "print(train_input_embedding)\n",
    "print(train_label_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 位置编码\n",
    "\n",
    "偶数位置用sin，奇数位置用cos：\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{PE}_{pos, 2i} &= \\sin{(pos/10000^{2i/d_{\\text{model}}})} \\\\\n",
    "\\text{PE}_{pos, 2i+1} &= \\cos{(pos/10000^{2i/d_{\\text{model}}})}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "其中，pos表示行，i表示列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mat = torch.arange(MAX_SENTENCE_LEN).reshape((-1, 1))\n",
    "i_mat = torch.arange(0, MODEL_DIM, 2).reshape((1, -1))\n",
    "frac_bottom = torch.pow(10000, i_mat / MODEL_DIM)\n",
    "op = pos_mat / frac_bottom\n",
    "\n",
    "pe = torch.zeros(MAX_SENTENCE_LEN, MODEL_DIM)\n",
    "# 偶数列\n",
    "pe[:, 0::2] = torch.sin(op)\n",
    "# 基数列\n",
    "pe[:, 1::2] = torch.cos(op)\n",
    "\n",
    "print(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_embedding = nn.Embedding(MAX_SENTENCE_LEN, MODEL_DIM)\n",
    "pe_embedding.weight = nn.Parameter(pe, requires_grad=False)\n",
    "\n",
    "print(pe_embedding.weight)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
