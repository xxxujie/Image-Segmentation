{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from torch import nn\n",
    "from torch.nn import functional as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批次大小\n",
    "BATCH_SIZE = 2\n",
    "# 单词表大小\n",
    "TOKEN_CNT = 8\n",
    "# 句子的最大长度\n",
    "MAX_INPUT_LEN = 5\n",
    "MAX_LABEL_LEN = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征维度大小\n",
    "FEATURE_DIM = 8  # 原论文是512，这里为了直观定义成8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 手写训练数据集（演示用）\n",
    "\n",
    "### 2.1 生成句子长度列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4], dtype=torch.int32)\n",
      "tensor([4, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 输入数据为一个批次的（2个）句子，长度分别为2,4\n",
    "train_input_lengths = torch.Tensor([2, 4]).to(torch.int32)\n",
    "# 标签数据为一个批次的（2个）句子，长度分别为4,3\n",
    "train_label_lengths = torch.Tensor([4, 3]).to(torch.int32)\n",
    "\n",
    "print(train_input_lengths)\n",
    "print(train_label_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 随机生成句子\n",
    "\n",
    "句子由token组成，为了方便，这里的token就是一个int数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1, 5, 0, 0, 0]), tensor([1, 1, 7, 2, 0])]\n",
      "[tensor([3, 7, 3, 6, 0]), tensor([3, 7, 4, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "train_input = []\n",
    "for length in train_input_lengths:\n",
    "    # 按照长度随机生成句子\n",
    "    input = torch.randint(1, TOKEN_CNT, (length,))\n",
    "    # 将句子padding到最大长度\n",
    "    input = TF.pad(input, (0, MAX_INPUT_LEN - length))\n",
    "    train_input.append(input)\n",
    "\n",
    "train_label = []\n",
    "for length in train_label_lengths:\n",
    "    # 按照长度随机生成句子\n",
    "    label = torch.randint(1, TOKEN_CNT, (length,))\n",
    "    # 将句子padding到最大长度\n",
    "    label = TF.pad(label, (0, MAX_INPUT_LEN - length))\n",
    "    train_label.append(label)\n",
    "\n",
    "print(train_input)\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 把多个句子拼接构成输入矩阵和标签矩阵\n",
    "\n",
    "#### 2.4 先把每个句子变成二维的矩阵形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1, 5, 0, 0, 0]]), tensor([[1, 1, 7, 2, 0]])]\n",
      "[tensor([[3, 7, 3, 6, 0]]), tensor([[3, 7, 4, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "for index, value in enumerate(train_input):\n",
    "    value = torch.squeeze(value)\n",
    "    train_input[index] = torch.unsqueeze(value, dim=0)\n",
    "\n",
    "for index, value in enumerate(train_label):\n",
    "    value = torch.squeeze(value)\n",
    "    train_label[index] = torch.unsqueeze(value, dim=0)\n",
    "\n",
    "print(train_input)\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 再把所有矩阵拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 5, 0, 0, 0],\n",
      "        [1, 1, 7, 2, 0]])\n",
      "tensor([[3, 7, 3, 6, 0],\n",
      "        [3, 7, 4, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "train_input = torch.cat(train_input, dim=0)\n",
    "train_label = torch.cat(train_label, dim=0)\n",
    "\n",
    "print(train_input)\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 构造Embedding\n",
    "\n",
    "### 3.1 获取Embedding表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-3.7422e-01,  6.7983e-01,  6.0466e-01,  5.3488e-01,  5.1561e-01,\n",
      "          3.8306e-01,  6.5662e-01, -6.1498e-02],\n",
      "        [-1.7563e-01,  1.7750e+00, -6.9537e-02, -7.2879e-01,  1.6425e-01,\n",
      "         -1.5033e+00,  4.5809e-01, -1.4157e-01],\n",
      "        [ 9.2038e-02,  3.8760e-01, -5.3536e-01,  4.4507e-01, -4.2964e-01,\n",
      "         -9.5809e-01,  3.2246e-02,  8.2909e-01],\n",
      "        [ 1.6994e-03, -6.3266e-01, -2.7601e-01, -1.8993e+00, -8.6873e-01,\n",
      "         -7.3563e-01,  1.4079e+00, -1.6753e-01],\n",
      "        [-1.6914e+00, -6.0648e-01, -1.2424e+00,  1.7541e+00,  2.4180e-02,\n",
      "         -7.3011e-01,  2.6621e-03, -1.9021e+00],\n",
      "        [ 4.1080e-01,  3.2339e+00,  5.5507e-01, -2.8856e+00,  1.6935e-01,\n",
      "         -2.4707e-01, -4.7805e-01, -4.4455e-01],\n",
      "        [-9.7700e-01, -4.0286e-01,  3.1571e-01,  3.1108e-01,  8.5578e-01,\n",
      "          8.1497e-01, -2.5994e-01,  1.2496e-01],\n",
      "        [ 4.3483e-01, -3.8921e-01,  1.0953e+00,  3.9218e-01, -2.8754e-01,\n",
      "         -3.2304e-01,  7.3571e-01, -1.3373e+00],\n",
      "        [-2.1222e+00, -2.0988e+00, -3.2507e-01,  9.8273e-02, -5.9892e-02,\n",
      "          1.2436e+00,  5.4987e-01,  3.6558e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.5029,  0.2149, -0.2790,  0.7935,  0.1460, -0.9299,  0.2808,  0.4017],\n",
      "        [-0.6299,  0.5734,  0.0478,  0.8225,  0.5732,  0.5928,  0.0818,  0.8647],\n",
      "        [ 0.8007,  0.0920, -0.5131,  0.1713,  0.4942,  0.1825,  0.3334,  0.7072],\n",
      "        [-0.2452,  0.1869,  0.3548, -1.4753, -0.8930, -0.0835,  0.6371, -0.1662],\n",
      "        [-0.8917, -0.1492,  0.9886,  1.0163,  0.5687,  1.1701, -0.3836, -2.6445],\n",
      "        [-1.1754, -1.2686, -2.6626,  0.4534,  1.1753, -0.4161, -0.2876, -0.3570],\n",
      "        [ 0.0460, -0.3349,  0.3584,  2.1211,  2.1345, -0.6189,  0.8944,  0.1423],\n",
      "        [-1.0499, -0.6346, -1.3442,  1.0312, -0.3027,  0.4850,  0.1104,  0.5832],\n",
      "        [-0.3552,  0.9729, -0.1954, -1.6920,  1.1536, -0.6696, -1.8100,  0.9520]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 加一个Embedding数量是为了给pad出来的0\n",
    "input_embedding_table = nn.Embedding(TOKEN_CNT + 1, FEATURE_DIM)\n",
    "lable_embedding_table = nn.Embedding(TOKEN_CNT + 1, FEATURE_DIM)\n",
    "print(input_embedding_table.weight)\n",
    "print(lable_embedding_table.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 将输入和输出的token转为Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1756,  1.7750, -0.0695, -0.7288,  0.1643, -1.5033,  0.4581,\n",
      "          -0.1416],\n",
      "         [ 0.4108,  3.2339,  0.5551, -2.8856,  0.1693, -0.2471, -0.4781,\n",
      "          -0.4445],\n",
      "         [-0.3742,  0.6798,  0.6047,  0.5349,  0.5156,  0.3831,  0.6566,\n",
      "          -0.0615],\n",
      "         [-0.3742,  0.6798,  0.6047,  0.5349,  0.5156,  0.3831,  0.6566,\n",
      "          -0.0615],\n",
      "         [-0.3742,  0.6798,  0.6047,  0.5349,  0.5156,  0.3831,  0.6566,\n",
      "          -0.0615]],\n",
      "\n",
      "        [[-0.1756,  1.7750, -0.0695, -0.7288,  0.1643, -1.5033,  0.4581,\n",
      "          -0.1416],\n",
      "         [-0.1756,  1.7750, -0.0695, -0.7288,  0.1643, -1.5033,  0.4581,\n",
      "          -0.1416],\n",
      "         [ 0.4348, -0.3892,  1.0953,  0.3922, -0.2875, -0.3230,  0.7357,\n",
      "          -1.3373],\n",
      "         [ 0.0920,  0.3876, -0.5354,  0.4451, -0.4296, -0.9581,  0.0322,\n",
      "           0.8291],\n",
      "         [-0.3742,  0.6798,  0.6047,  0.5349,  0.5156,  0.3831,  0.6566,\n",
      "          -0.0615]]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[-0.2452,  0.1869,  0.3548, -1.4753, -0.8930, -0.0835,  0.6371,\n",
      "          -0.1662],\n",
      "         [-1.0499, -0.6346, -1.3442,  1.0312, -0.3027,  0.4850,  0.1104,\n",
      "           0.5832],\n",
      "         [-0.2452,  0.1869,  0.3548, -1.4753, -0.8930, -0.0835,  0.6371,\n",
      "          -0.1662],\n",
      "         [ 0.0460, -0.3349,  0.3584,  2.1211,  2.1345, -0.6189,  0.8944,\n",
      "           0.1423],\n",
      "         [ 0.5029,  0.2149, -0.2790,  0.7935,  0.1460, -0.9299,  0.2808,\n",
      "           0.4017]],\n",
      "\n",
      "        [[-0.2452,  0.1869,  0.3548, -1.4753, -0.8930, -0.0835,  0.6371,\n",
      "          -0.1662],\n",
      "         [-1.0499, -0.6346, -1.3442,  1.0312, -0.3027,  0.4850,  0.1104,\n",
      "           0.5832],\n",
      "         [-0.8917, -0.1492,  0.9886,  1.0163,  0.5687,  1.1701, -0.3836,\n",
      "          -2.6445],\n",
      "         [ 0.5029,  0.2149, -0.2790,  0.7935,  0.1460, -0.9299,  0.2808,\n",
      "           0.4017],\n",
      "         [ 0.5029,  0.2149, -0.2790,  0.7935,  0.1460, -0.9299,  0.2808,\n",
      "           0.4017]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_input = input_embedding_table(train_input)\n",
    "train_label = lable_embedding_table(train_label)\n",
    "\n",
    "print(train_input)\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 位置编码\n",
    "\n",
    "偶数位置用sin，奇数位置用cos：\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{PE}_{pos, 2i} &= \\sin{(pos/10000^{2i/d_{\\text{model}}})} \\\\\n",
    "\\text{PE}_{pos, 2i+1} &= \\cos{(pos/10000^{2i/d_{\\text{model}}})}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "其中，pos表示行，i表示列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "          9.9920e-01,  4.0000e-03,  9.9999e-01]])\n"
     ]
    }
   ],
   "source": [
    "pos_mat = torch.arange(MAX_INPUT_LEN).reshape((-1, 1))\n",
    "i_mat = torch.arange(0, FEATURE_DIM, 2).reshape((1, -1))\n",
    "frac_bottom = torch.pow(10000, i_mat / FEATURE_DIM)\n",
    "op = pos_mat / frac_bottom\n",
    "\n",
    "pe = torch.zeros(MAX_INPUT_LEN, FEATURE_DIM)\n",
    "# 偶数列\n",
    "pe[:, 0::2] = torch.sin(op)\n",
    "# 基数列\n",
    "pe[:, 1::2] = torch.cos(op)\n",
    "\n",
    "print(pe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
