{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from torch import nn\n",
    "from torch.nn import functional as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批次大小\n",
    "BATCH_SIZE = 2\n",
    "# 单词表大小\n",
    "TOKEN_CNT = 8\n",
    "# 句子的最大长度\n",
    "MAX_INPUT_LEN = 5\n",
    "MAX_LABEL_LEN = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征维度大小\n",
    "FEATURE_DIM = 8  # 原论文是512，这里为了直观定义成8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 手写训练数据集（演示用）\n",
    "\n",
    "### 2.1 生成句子长度列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4], dtype=torch.int32)\n",
      "tensor([4, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 输入数据为一个批次的（2个）句子，长度分别为2,4\n",
    "train_input_lengths = torch.Tensor([2, 4]).to(torch.int32)\n",
    "# 标签数据为一个批次的（2个）句子，长度分别为4,3\n",
    "train_label_lengths = torch.Tensor([4, 3]).to(torch.int32)\n",
    "\n",
    "print(train_input_lengths)\n",
    "print(train_label_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 随机生成句子\n",
    "\n",
    "句子由token组成，为了方便，这里的token就是一个int数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([7, 2, 0, 0, 0]), tensor([2, 4, 1, 6, 0])]\n",
      "[tensor([2, 3, 4, 6, 0]), tensor([2, 2, 2, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "train_input = []\n",
    "for length in train_input_lengths:\n",
    "    # 按照长度随机生成句子\n",
    "    input = torch.randint(1, TOKEN_CNT, (length,))\n",
    "    # 将句子padding到最大长度\n",
    "    input = TF.pad(input, (0, MAX_INPUT_LEN - length))\n",
    "    train_input.append(input)\n",
    "\n",
    "train_label = []\n",
    "for length in train_label_lengths:\n",
    "    # 按照长度随机生成句子\n",
    "    label = torch.randint(1, TOKEN_CNT, (length,))\n",
    "    # 将句子padding到最大长度\n",
    "    label = TF.pad(label, (0, MAX_INPUT_LEN - length))\n",
    "    train_label.append(label)\n",
    "\n",
    "print(train_input)\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 把多个句子拼接构成输入矩阵和标签矩阵\n",
    "\n",
    "#### 2.4 先把每个句子变成二维的矩阵形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[7, 2, 0, 0, 0]]), tensor([[2, 4, 1, 6, 0]])]\n",
      "[tensor([[2, 3, 4, 6, 0]]), tensor([[2, 2, 2, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "for index, value in enumerate(train_input):\n",
    "    value = torch.squeeze(value)\n",
    "    train_input[index] = torch.unsqueeze(value, dim=0)\n",
    "\n",
    "for index, value in enumerate(train_label):\n",
    "    value = torch.squeeze(value)\n",
    "    train_label[index] = torch.unsqueeze(value, dim=0)\n",
    "\n",
    "print(train_input)\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 再把所有矩阵拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 2, 0, 0, 0],\n",
      "        [2, 4, 1, 6, 0]])\n",
      "tensor([[2, 3, 4, 6, 0],\n",
      "        [2, 2, 2, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "train_input = torch.cat(train_input, dim=0)\n",
    "train_label = torch.cat(train_label, dim=0)\n",
    "\n",
    "print(train_input)\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 构造Embedding\n",
    "\n",
    "### 3.1 获取Embedding表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2832,  0.6215, -0.2966,  0.2469, -1.4012,  1.3872,  1.5670, -1.3480],\n",
      "        [-0.3355, -0.9772,  0.7343, -0.9183,  0.8904, -0.6822, -1.5822, -0.2810],\n",
      "        [-0.0872, -0.6031,  1.0068,  0.5891,  1.3712,  1.3158, -0.8492, -1.4613],\n",
      "        [ 1.8162, -0.2163, -0.2869, -0.1422, -0.8720, -1.2279, -0.3913,  1.2112],\n",
      "        [ 1.7345,  0.8102, -0.3895,  0.0219, -0.6082, -0.3863,  0.0518,  0.3908],\n",
      "        [ 0.7476,  0.0781,  0.5560,  0.7909, -0.0520,  0.4376,  0.2725,  0.1683],\n",
      "        [-0.0133,  0.2394,  0.1681,  0.3647, -0.5152, -0.2608, -0.6800,  1.4019],\n",
      "        [-2.2764,  0.8400,  0.2562,  0.6510,  1.9867,  0.9069, -0.2101,  0.1026],\n",
      "        [-0.2368, -0.4331, -0.2906,  0.3106,  0.2850, -0.9016,  0.3648, -0.5448]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5264, -2.1155,  0.6598,  0.6040,  0.1247, -0.8928,  0.8802, -0.2922],\n",
      "        [-1.8382, -1.1926, -1.0516,  0.3331,  0.0288, -0.3170,  0.6016, -0.0354],\n",
      "        [-0.5501,  0.3884, -0.5186,  0.6050,  0.2306,  0.5557, -0.0566,  0.4016],\n",
      "        [ 0.3307, -0.6645, -0.7146,  0.1825,  0.5147,  0.6551, -1.1273, -1.1570],\n",
      "        [ 0.7216,  1.0472,  0.5817,  1.3281,  0.6330, -1.4378,  1.0464,  0.9630],\n",
      "        [ 0.3108,  0.2979,  0.3838, -1.6680,  0.2784,  1.7199, -0.5664, -0.1329],\n",
      "        [-1.2965,  0.1438,  1.6822, -2.0767, -1.1833,  2.4567,  0.7699, -0.0830],\n",
      "        [ 0.6390, -1.6991, -1.6045, -0.4554,  0.2076, -1.2325,  0.4627,  0.6030],\n",
      "        [ 0.1828,  0.5355, -0.0981, -0.0482,  0.2196, -0.6086,  1.1251, -0.6729]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 加一个Embedding数量是为了给pad出来的0\n",
    "input_embedding_table = nn.Embedding(TOKEN_CNT + 1, FEATURE_DIM)\n",
    "lable_embedding_table = nn.Embedding(TOKEN_CNT + 1, FEATURE_DIM)\n",
    "print(input_embedding_table.weight)\n",
    "print(lable_embedding_table.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 将输入和输出的token转为Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.2764,  0.8400,  0.2562,  0.6510,  1.9867,  0.9069, -0.2101,\n",
      "           0.1026],\n",
      "         [-0.0872, -0.6031,  1.0068,  0.5891,  1.3712,  1.3158, -0.8492,\n",
      "          -1.4613],\n",
      "         [-0.2832,  0.6215, -0.2966,  0.2469, -1.4012,  1.3872,  1.5670,\n",
      "          -1.3480],\n",
      "         [-0.2832,  0.6215, -0.2966,  0.2469, -1.4012,  1.3872,  1.5670,\n",
      "          -1.3480],\n",
      "         [-0.2832,  0.6215, -0.2966,  0.2469, -1.4012,  1.3872,  1.5670,\n",
      "          -1.3480]],\n",
      "\n",
      "        [[-0.0872, -0.6031,  1.0068,  0.5891,  1.3712,  1.3158, -0.8492,\n",
      "          -1.4613],\n",
      "         [ 1.7345,  0.8102, -0.3895,  0.0219, -0.6082, -0.3863,  0.0518,\n",
      "           0.3908],\n",
      "         [-0.3355, -0.9772,  0.7343, -0.9183,  0.8904, -0.6822, -1.5822,\n",
      "          -0.2810],\n",
      "         [-0.0133,  0.2394,  0.1681,  0.3647, -0.5152, -0.2608, -0.6800,\n",
      "           1.4019],\n",
      "         [-0.2832,  0.6215, -0.2966,  0.2469, -1.4012,  1.3872,  1.5670,\n",
      "          -1.3480]]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[-0.5501,  0.3884, -0.5186,  0.6050,  0.2306,  0.5557, -0.0566,\n",
      "           0.4016],\n",
      "         [ 0.3307, -0.6645, -0.7146,  0.1825,  0.5147,  0.6551, -1.1273,\n",
      "          -1.1570],\n",
      "         [ 0.7216,  1.0472,  0.5817,  1.3281,  0.6330, -1.4378,  1.0464,\n",
      "           0.9630],\n",
      "         [-1.2965,  0.1438,  1.6822, -2.0767, -1.1833,  2.4567,  0.7699,\n",
      "          -0.0830],\n",
      "         [-0.5264, -2.1155,  0.6598,  0.6040,  0.1247, -0.8928,  0.8802,\n",
      "          -0.2922]],\n",
      "\n",
      "        [[-0.5501,  0.3884, -0.5186,  0.6050,  0.2306,  0.5557, -0.0566,\n",
      "           0.4016],\n",
      "         [-0.5501,  0.3884, -0.5186,  0.6050,  0.2306,  0.5557, -0.0566,\n",
      "           0.4016],\n",
      "         [-0.5501,  0.3884, -0.5186,  0.6050,  0.2306,  0.5557, -0.0566,\n",
      "           0.4016],\n",
      "         [-0.5264, -2.1155,  0.6598,  0.6040,  0.1247, -0.8928,  0.8802,\n",
      "          -0.2922],\n",
      "         [-0.5264, -2.1155,  0.6598,  0.6040,  0.1247, -0.8928,  0.8802,\n",
      "          -0.2922]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_input = input_embedding_table(train_input)\n",
    "train_label = lable_embedding_table(train_label)\n",
    "\n",
    "print(train_input)\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 位置编码\n",
    "\n",
    "偶数位置用sin，奇数位置用cos：\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{PE}_{pos, 2i} &= \\sin{(pos/10000^{2i/d_{\\text{model}}})} \\\\\n",
    "\\text{PE}_{pos, 2i+1} &= \\cos{(pos/10000^{2i/d_{\\text{model}}})}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "其中，pos表示行，i表示列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
